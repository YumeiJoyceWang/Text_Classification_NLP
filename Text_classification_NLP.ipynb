{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Text Classification\n",
    "\n",
    "In this project we look at several ways of classifying texts:\n",
    "- Naive Bayes\n",
    "- Logistic Regression\n",
    "- Multinomial Regression\n",
    "\n",
    "We will use two datasets for binary label classification (sentiment analysis) and multinomial classification (topic analysis):\n",
    "- [IMDb movie review sentiment](http://ai.stanford.edu/~amaas/data/sentiment/)\n",
    "- [AG News topics](https://huggingface.co/datasets/ag_news)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "1xVv6McXodZG",
    "tags": []
   },
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "vMRiPTv7vHe2",
    "tags": []
   },
   "source": [
    "Import packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import classification_report\n",
    "import sys\n",
    "import traceback\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "qyRo0v0JIKTK",
    "tags": []
   },
   "source": [
    "# Functions for cleaning up raw texts and tokenizing the corpus\n",
    "\n",
    "We perform text preprocessing that includes: removing HTML tags, making text lower case, stemming, and disposing of stopwords.\n",
    "In the end, we will split the entire dataset into training, validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": false,
    "id": "JvfvVWHyIKTL",
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# Stemming the text\n",
    "def simple_stemmer(text):\n",
    "    ps=nltk.porter.PorterStemmer()\n",
    "    text= [ps.stem(word) for word in text]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_english = [\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]\n",
    "# print(stopwords_english)\n",
    "\n",
    "#removing the stopwords\n",
    "def remove_stopwords(text, stopword_list):\n",
    "    tokens = [token.strip() for token in text]\n",
    "    filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": false,
    "id": "NCXHv_tZIKTM",
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "def tokenize_and_clean(line, stem_and_remove_stop_words = True):\n",
    "\n",
    "    line = re.sub(r\"<.*?>\", \"\", line).strip() # remove all HTML tags\n",
    "    line = re.sub(r'[^a-zA-Z0-9]', ' ', line) # remove punc\n",
    "    line = line.lower().split()  # lower case\n",
    "    if stem_and_remove_stop_words:\n",
    "        line = remove_stopwords(line, stopwords_english)\n",
    "        line = simple_stemmer(line)\n",
    "\n",
    "    return line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "ohc6u_lDIKS-",
    "tags": []
   },
   "source": [
    "# Download and unpack the sentiment data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "zIklHox9vjMI",
    "tags": []
   },
   "source": [
    "We are using IMDb Dataset for binary sentiment classification that provides a set of 25K highly polar reviews for training, and 25K for testing\n",
    "(each set contains an equal number of positive and negative examples).\n",
    "\n",
    "Dataset folder structure is as follows:\n",
    "\n",
    "dataset/ \\\n",
    "├── test/ \\\n",
    "│     ├── pos/ \\\n",
    "│     ├── neg/ \\\n",
    "├── train/ \\\n",
    "      ├── pos/ \\\n",
    "      └── neg/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "id": "pxYJtzVj681M",
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# check if dataset is downloaded\n",
    "if not os.path.isfile('aclImdb_v1.tar'):\n",
    "    print(\"Downloading dataset...\")\n",
    "    !wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "    !gunzip aclImdb_v1.tar.gz\n",
    "    !tar -xvf aclImdb_v1.tar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "GRD03KapvnI0",
    "tags": []
   },
   "source": [
    "Load in the text from the folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text_from_folders(path, file_list, dataset, samples = 25000, stem_and_remove_stop_words = True):\n",
    "    \"\"\"Read set of files from given directory and save returned lines to list.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path : str\n",
    "        Absolute or relative path to given file (or set of files).\n",
    "    file_list: list\n",
    "        List of files names to read.\n",
    "    dataset: list\n",
    "        List that stores read lines.\n",
    "    samples: int\n",
    "        Number of samples in the output\n",
    "    \"\"\"\n",
    "    for i, file in enumerate(file_list):\n",
    "        if i >= samples:\n",
    "            break\n",
    "        with open(os.path.join(path, file), 'r', encoding='utf8') as text:\n",
    "            contents = text.read()\n",
    "            contents_tokenized = tokenize_and_clean(contents, stem_and_remove_stop_words=stem_and_remove_stop_words)\n",
    "            dataset.append(contents_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "_8KuCNoXIKTO",
    "tags": []
   },
   "source": [
    "# Creating training and test sets\n",
    "\n",
    "This creates four arrays:\n",
    "\n",
    "\n",
    "*   ```train_pos``` -- instances in the training set with positive sentiment labels\n",
    "*   ```train_neg``` -- instances in the training set with negative sentiment labels\n",
    "*   ```test_pos``` -- instances in the testing set with positive sentiment labels\n",
    "*   ```test_neg``` -- instances in the testing set with negative sentiment labels\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": false,
    "id": "4Nv5FvBIIKTO",
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# Path to dataset location\n",
    "path = 'aclImdb/'\n",
    "\n",
    "# Create lists that will contain read lines\n",
    "train_pos, train_neg, test_pos, test_neg = [], [], [], []\n",
    "\n",
    "# Create a dictionary of paths and lists that store lines (key: value = path: list)\n",
    "sets_dict = {'train/pos/': train_pos, 'train/neg/': train_neg,\n",
    "             'test/pos/': test_pos, 'test/neg/': test_neg}\n",
    "\n",
    "# Load the data\n",
    "for dataset in sets_dict:\n",
    "  file_list = [f for f in sorted(os.listdir(os.path.join(path, dataset))) if f.endswith('.txt')]\n",
    "  load_text_from_folders(os.path.join(path, dataset), file_list, sets_dict[dataset])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "3t5nCE_xpvx7",
    "tags": []
   },
   "source": [
    "Convert into Pandas dataframes. Pandas is a virtual spreadsheet with a programmatic API. A ```DataFrame``` is a spreadsheet. We will make a spreadsheet of training data and one for testing data and one with everything together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": false,
    "id": "kovNgXqIIKTP",
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# Concatenate training and testing examples into one dataset\n",
    "TRAIN = pd.concat([pd.DataFrame({'review': train_pos, 'label':1}),\n",
    "                     pd.DataFrame({'review': train_neg, 'label':0})],\n",
    "                     axis=0, ignore_index=True)\n",
    "\n",
    "TEST = pd.concat([pd.DataFrame({'review': test_pos, 'label':1}),\n",
    "                    pd.DataFrame({'review': test_neg, 'label':0})],\n",
    "                    axis=0, ignore_index=True)\n",
    "\n",
    "ALL = pd.concat([TRAIN, TEST])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": false,
    "id": "0xPnt_CDIKTP",
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1    17505\n",
       "0    17411\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": false,
    "id": "InVC2IvnIKTQ",
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[bromwel, high, cartoon, comedi, ran, time, pr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[bromwel, high, cartoon, comedi, ran, time, pr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[homeless, houseless, georg, carlin, state, is...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[brilliant, act, lesley, ann, warren, best, dr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[easili, underr, film, inn, brook, cannon, sur...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  label\n",
       "0  [bromwel, high, cartoon, comedi, ran, time, pr...      1\n",
       "1  [bromwel, high, cartoon, comedi, ran, time, pr...      1\n",
       "2  [homeless, houseless, georg, carlin, state, is...      1\n",
       "3  [brilliant, act, lesley, ann, warren, best, dr...      1\n",
       "4  [easili, underr, film, inn, brook, cannon, sur...      1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "3jP5U6usIKTR",
    "tags": []
   },
   "source": [
    "# Creating a vocabulary file\n",
    "\n",
    "Next, we have to build a vocabulary. This is effectively a look-up table where every unique word in your data set has a corresponding index (an integer).\n",
    "We do this as our machine learning model cannot operate on strings, but only numbers. Each index is used to construct a one-hot vector for each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": false,
    "id": "Oz6hQlCgqckU",
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self._word2index = {}\n",
    "        self._word2count = {}\n",
    "        self._index2word = {}\n",
    "        self._n_words = 0\n",
    "\n",
    "    def get_words(self):\n",
    "      return list(self._word2count.keys())\n",
    "\n",
    "    def num_words(self):\n",
    "      return self._n_words\n",
    "\n",
    "    def word2index(self, word):\n",
    "      return self._word2index[word]\n",
    "\n",
    "    def index2word(self, word):\n",
    "      return self._index2word[word]\n",
    "\n",
    "    def word2count(self, word):\n",
    "      return self._word2count[word]\n",
    "\n",
    "    def add_sentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.add_word(word)\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self._word2index:\n",
    "            self._word2index[word] = self._n_words\n",
    "            self._word2count[word] = 1\n",
    "            self._index2word[self._n_words] = word\n",
    "            self._n_words += 1\n",
    "        else:\n",
    "            self._word2count[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "UT7oI7pXw9wA",
    "tags": []
   },
   "source": [
    "Make a vocab object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": false,
    "id": "-FLxtGNgw79Q",
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "VOCAB = Vocab(\"imdb\")\n",
    "VOCAB_SIZE = 1000\n",
    "NUM_LABELS = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "ggqgPtCSxAG-",
    "tags": []
   },
   "source": [
    "Load the first ```n``` frequent words in the vocabulary. Do this by sorting by frequency and then truncating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": false,
    "id": "DAiKNELzwC2T",
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# Get word frequency counts\n",
    "word_freq_dict = {}   # key = word, value = frequency\n",
    "for review in ALL['review']:\n",
    "  for word in review:\n",
    "    if word in word_freq_dict:\n",
    "      word_freq_dict[word] += 1\n",
    "    else:\n",
    "      word_freq_dict[word] = 1\n",
    "\n",
    "# Get a list of (word, freq) tuples sorted by frequency\n",
    "kv_list = []  # list of word-freq tuples so can sort\n",
    "for (k,v) in word_freq_dict.items():\n",
    "  kv_list.append((k,v))\n",
    "sorted_kv_list = sorted(kv_list, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Load top n words in to vocab object\n",
    "for word, freq in sorted_kv_list[:VOCAB_SIZE]:\n",
    "  VOCAB.add_word(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "hvh0yHT_IKTi",
    "tags": []
   },
   "source": [
    "# Naive Bayes\n",
    "Naive Bayes Algorithm is based on the Bayes Rule which describes the probability of an event,\n",
    "based on prior knowledge of conditions that might be related to the event.\n",
    "\n",
    "According to Bayes theorem:\n",
    "\n",
    "\n",
    "```Posterior = likelihood * proposition/evidence```\n",
    "\n",
    "or\n",
    "\n",
    "```P(A|B) = P(B|A) * P(A)/P(B)```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "vELAfeWDRrNq",
    "tags": []
   },
   "source": [
    "Using word presence as features, create an array of features for each review. Each review will thus be an array of size ```len(vocab)``` where each index in the array is a token number and the value in that position is whether the token is present in the review. There will be ```num_rows``` arrays, making a ```num_rows x len(vocab)``` 2D array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "DFf6zYmozBtc",
    "tags": []
   },
   "source": [
    "This function creates a bag of words. It returns a vector where each element is a count of the words in the sentence corresponding to the word index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": false,
    "id": "DKCwfenwIKTw",
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "def make_bow(sentence):\n",
    "    vec = torch.zeros(VOCAB_SIZE, dtype=torch.float64)\n",
    "    for word in sentence:\n",
    "        if word not in VOCAB.get_words():\n",
    "            continue\n",
    "        vec[VOCAB.word2index(word)] += 1\n",
    "    return vec.view(1, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "NHweI8gPzOdh",
    "tags": []
   },
   "source": [
    "Prepare data ```X_TRAIN``` is a 2D array of size ```num_reviews x vocab_size``` that contains training data. Each row will be a bag of words, except each index contains a 1 or 0 based on word presence in the example. Each row is a vector of features $\\phi_1 ... \\phi_{|V|}$ assumed to be independent, where $|V|$ is size of the vocabulary. We don't need to know what the features are, only whether they are present in each example in the training set.\n",
    "\n",
    "```X_TEST``` is the same as above but containing testing data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": false,
    "id": "c30F5ZwyIKTw",
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# Vectorize text reviews to numbers\n",
    "# Make empty vectors\n",
    "X_TRAIN = np.zeros((len(TRAIN), VOCAB_SIZE))\n",
    "X_TEST = np.zeros((len(TEST), VOCAB_SIZE))\n",
    "\n",
    "# Load in frequency counts\n",
    "for i, row in TRAIN.iterrows():\n",
    "    X_TRAIN[i] = np.array(make_bow(row['review'])) > 0 # The > 0 converts to presence instead of counts\n",
    "\n",
    "for i, row in TEST.iterrows():\n",
    "    X_TEST[i] = np.array(make_bow(row['review'])) > 0 # The > 0 converts to presence instead of counts\n",
    "\n",
    "# The labels\n",
    "Y_TRAIN = np.array(TRAIN['label'])\n",
    "Y_TEST = np.array(TEST['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will compute probabilities over the training data and then apply those probabilities to the testing examples. Use the Bayes formula to compute $P_{\\rm test}(L_{+}|\\phi_{0:|V|})$ and $P_{\\rm test}(L_{-}|\\phi_{0:|V|})$ for each review. Classify examples based on whether one probability is higher than another. That is, $sign(P_{\\rm test}(L_{+}|\\phi_{0:|V|}) - P_{\\rm test}(L_{-}|\\phi_{0:|V|}))$ indicates a positive review when greater than 0 and a negative review when less than 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34916, 1000)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_TRAIN.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34916,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_TRAIN.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Compute the positive label condition:\n",
    "$P(L_{+}|\\phi_{0:|V|}) = P(\\phi_{0:|V|}|L_{+})P(L_{+}) / P(\\phi_{0:|V|})$ \n",
    "and the negative label condition:\n",
    "$P(L_{-}|\\phi_{0:|V|}) = P(\\phi_{0:|V|}|L_{-})P(L_{-}) / P(\\phi_{0:|V|})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_given_features(x_train, y_train):\n",
    "  log_probs = np.array([0] * x_train.shape[1])\n",
    "  # Likelihood and prior probability for positive labels\n",
    "  num_pos = np.sum(y_train==1)\n",
    "  x_train_pos = x_train[y_train==1]\n",
    "  likelihood_pos = (1+np.sum(x_train_pos,axis=0))/(1+num_pos)\n",
    "  prior_pos = np.mean(y_train==1)\n",
    "\n",
    "  # Likelihood and prior probability for negative labels\n",
    "  num_neg = np.sum(y_train==0)\n",
    "  x_train_neg = x_train[y_train==0]\n",
    "  likelihood_neg = (1+np.sum(x_train_neg,axis=0))/(1+num_neg)\n",
    "  prior_neg = np.mean(y_train==0)\n",
    "  \n",
    "  # calculate the frequency of each feature (the sum of each column)\n",
    "  col_sums = np.sum(x_train, axis=0)\n",
    "  # total number of features and then add 1 to smooth\n",
    "  total = np.sum(x_train)\n",
    "  # denominator: probability of each feature, add 1 to smooth\n",
    "  evidence = (1+col_sums) / (1+total)\n",
    "    \n",
    "  # log scale posterior probs\n",
    "  log_pos_probs = np.log(likelihood_pos * prior_pos / evidence)\n",
    "  log_neg_probs = np.log(likelihood_neg * prior_neg / evidence)\n",
    "  return log_pos_probs, log_neg_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_probs, neg_probs = prob_given_features(X_TRAIN, Y_TRAIN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Make a label prediction. Subtract (in log scale) the positive from the negative. If the result is greater than zero then it is a prediction of `+` label. If the result is less thn zero then we make a prediction of `-` label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true,
    "id": "xaz7KBdd_CD1",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7b4cde57fd68ac07",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "def naive_bayes(x, pos_probs, neg_probs):\n",
    "  label = 0\n",
    "  x_pos_probs = np.dot(x, pos_probs)\n",
    "  x_neg_probs = np.dot(x, neg_probs)\n",
    "  # Subtract the positive from the negative\n",
    "  x_probs = np.sum(x_pos_probs) - np.sum(x_neg_probs)\n",
    "  if x_probs > 0:\n",
    "      label = 1\n",
    "  else:\n",
    "      lable = 0\n",
    "  return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_naive_bayes(x_train, y_train, x_test, y_test):\n",
    "    try:\n",
    "        # Get the positive and negative feature probabilities\n",
    "        pos_probs, neg_probs  = prob_given_features(x_train, y_train)\n",
    "        correct = 0  # How many tests are correct\n",
    "        # Iterate through the test set\n",
    "        for x, y in zip(x_test, y_test):\n",
    "            # Get the naive_bayes label\n",
    "            label = naive_bayes(x, pos_probs, neg_probs)\n",
    "            # Compare the label against the true label\n",
    "            correct = correct + int(label == y)\n",
    "        print('Accuracy: ', correct / x_test.shape[0])\n",
    "    except Exception as e:\n",
    "        print('Error during execution of Test:')\n",
    "        # print traceback\n",
    "        traceback.print_exc()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8328486681400489\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes Test\n",
    "test_naive_bayes(X_TRAIN, Y_TRAIN, X_TEST, Y_TEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "DWKJNb2CXhe5",
    "tags": []
   },
   "source": [
    "Reload the data, but use word counts instead of word presence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "editable": false,
    "id": "sBjsMbDSIKT6",
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# Randomize the data\n",
    "TRAIN = TRAIN.sample(frac=1).reset_index(drop=True)\n",
    "TEST = TEST.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Vectorize text reviews to numbers\n",
    "X_TRAIN = np.zeros((len(TRAIN), VOCAB_SIZE))\n",
    "X_TEST = np.zeros((len(TEST), VOCAB_SIZE))\n",
    "\n",
    "for i, row in TRAIN.iterrows():\n",
    "  X_TRAIN[i] = np.array(make_bow(row['review']))\n",
    "\n",
    "for i, row in TEST.iterrows():\n",
    "  X_TEST[i] = np.array(make_bow(row['review']))\n",
    "\n",
    "Y_TRAIN = np.array(TRAIN['label'])\n",
    "Y_TEST = np.array(TEST['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a logistic classifier torch neural network.\n",
    "\n",
    "The net will take an arbitrary number of outputs, but for binary logistic regression, only one is needed because the single output neuron can take a value that is between 0 and 1, with 0 meaning negative sentiment and 1 meaning positive sentiment. There should only be as many parameters as ```num_features x (num_labels-1)``` in binary logistic regression and ```num_features x num_labels``` for multinomial logistic regression.\n",
    "\n",
    "The input will be a one-hot vector of size `vocab_size`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression - The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "editable": true,
    "id": "yeJnVl7NIKT8",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c6124df74ac9d8f1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# Defining neural network structure\n",
    "class BoWClassifier(nn.Module):  # inheriting from nn.Module!\n",
    "\n",
    "  def __init__(self, num_labels, vocab_size):\n",
    "    super(BoWClassifier, self).__init__()\n",
    "    # initialize one linear layer\n",
    "    # input dimension is vocab_size and the output is num_labels\n",
    "    self.linear = nn.Linear(vocab_size, num_labels)\n",
    "\n",
    "  def forward(self, bow_vec):\n",
    "    # Pass the input through the linear layer, then pass that through sigmoid (for non-linearity).\n",
    "    out = nn.functional.sigmoid(self.linear(bow_vec))\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "editable": false,
    "id": "mgPYvkocIKT8",
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "# Use one label because the head can signify a 1 or 0 because of the sigmoid.\n",
    "bow_nn_model = BoWClassifier(NUM_LABELS-1, VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "xaBdfHfDQ5oI",
    "tags": []
   },
   "source": [
    "This function should return two tensors. The first, containing training data, shoud be of size ```batch_size x vocab_size``` for the ```i```th batch. The second should be a list of labels of size ```batch_size```. Both tensors should be of type ```dtype=torch.float```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "editable": true,
    "id": "RPxjUwUTPZTq",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-abf634c3a00755c7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "def get_batch(i, batch_size, x_data, y_data):\n",
    "  x = torch.tensor(x_data[batch_size * i : batch_size * (i+1)], dtype=torch.float)\n",
    "  y = torch.tensor(y_data[batch_size * i : batch_size * (i+1)], dtype=torch.float)\n",
    "  return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "editable": false,
    "id": "paJEFuigIKT-",
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "def train(model, train_data, test_data, epochs, batch_size):\n",
    "  n_iter = len(train_data) // batch_size\n",
    "  print(n_iter, 'batches per epoch')\n",
    "  # Loss Function\n",
    "  loss_function = nn.BCELoss()\n",
    "  # Optimizer initlialization\n",
    "  optimizer = optim.SGD(bow_nn_model.parameters(), lr=0.1)\n",
    "\n",
    "  for epoch in range(epochs):\n",
    "    # Make BOW vector for input features and target label\n",
    "    for i in range(n_iter):\n",
    "      x, y = get_batch(i, batch_size, train_data, test_data)\n",
    "\n",
    "      # Step 3. Run the forward pass.\n",
    "      y_hat = model(x)\n",
    "      y_hat = y_hat.reshape(-1)\n",
    "\n",
    "      # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "      loss = loss_function(y_hat,y)\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      if (epoch+1)%10 == 0 and (i+1) == n_iter:\n",
    "        print('epoch:', epoch+1,',loss =',loss.item(), ', training accuracy =',(torch.round(y_hat)==y).float().mean())\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "editable": true,
    "id": "gzD90s3KQgJd",
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# It's ok to modify this cell.\n",
    "BATCH_SIZE = 100\n",
    "N_EPOCHS = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "id": "Tv8Lqg8T949T",
    "scrolled": true,
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    bow_nn_model = train(bow_nn_model, X_TRAIN, Y_TRAIN, N_EPOCHS, BATCH_SIZE)\n",
    "except:\n",
    "    print(\"Training failed. Please check your code.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_accuracy_lr(TEST, bow_nn_model):\n",
    "    try:\n",
    "        bow_nn_predictions = []\n",
    "        with torch.no_grad():\n",
    "            for index, row in TEST.iterrows():\n",
    "                bow_vec = make_bow(row['review'])\n",
    "                probs = bow_nn_model(bow_vec.float())\n",
    "                pred = 1 if probs[0][0] > 0.5 else 0\n",
    "                bow_nn_predictions.append(pred)\n",
    "        accuracy = round((bow_nn_predictions == TEST['label']).mean(), 1)\n",
    "        print(classification_report(TEST['label'], bow_nn_predictions))\n",
    "        print('Accuracy: ', accuracy)\n",
    "    except Exception as e:\n",
    "        print('Error during execution of Test:')\n",
    "        # print traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.86      0.86     16694\n",
      "           1       0.87      0.86      0.87     17694\n",
      "\n",
      "    accuracy                           0.86     34388\n",
      "   macro avg       0.86      0.86      0.86     34388\n",
      "weighted avg       0.86      0.86      0.86     34388\n",
      "\n",
      "Accuracy:  0.9\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "test_model_accuracy_lr(TEST, bow_nn_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "DphmlDCk5sr5",
    "tags": []
   },
   "source": [
    "# Multinomial Regression\n",
    "\n",
    "Load data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "id": "SIiy4PBJVh7h",
    "scrolled": true,
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "editable": false,
    "id": "GrcwUiN-VfSf",
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "gqcveFex52Fy",
    "tags": []
   },
   "source": [
    "Unlike earlier, we will use a pre-defined set of embeddings, called [GLoVe](https://nlp.stanford.edu/projects/glove/). GLoVe replaces every word with a 100-dimensional vector of floating point values. The advantage of this is that words with similar semantic meanings will have similar vectors. This is important because the vocabulary size of the corpus we will use is 400,000.\n",
    "\n",
    "For the assigment, instead of getting a one-hot vector for each word, the neural network will get a `batch_size x num_words x 100` tensor containing floating point values.\n",
    "\n",
    "Download the GLoVe embedding vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "editable": false,
    "id": "7zIXPea4pZCi",
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "import gensim.downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "editable": false,
    "id": "-_CNat0Qpf_b",
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "glove_vectors = gensim.downloader.load('glove-wiki-gigaword-100')\n",
    "VOCAB_SIZE = len(glove_vectors.vectors)\n",
    "EMBEDDING_DIM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data_train = load_dataset(\"ag_news\", split=\"train\").shuffle()\n",
    "news_data_test = load_dataset(\"ag_news\", split=\"test\").shuffle()\n",
    "NEWS_TRAIN = pd.DataFrame(news_data_train)\n",
    "NEWS_TEST = pd.DataFrame(news_data_test)\n",
    "NUM_LABELS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "editable": false,
    "id": "pmx1OElZaAki",
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PeopleSoft, SAP make bid for manufacturing dol...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Socialites unite dolphin groups Dolphin groups...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Israel Destroys Refugee Homes, Kills One GAZA ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EU set to launch 'transit camps' EU ministers ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Earthquakes Shake Central Japan; Bullet Train ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  PeopleSoft, SAP make bid for manufacturing dol...      3\n",
       "1  Socialites unite dolphin groups Dolphin groups...      3\n",
       "2  Israel Destroys Refugee Homes, Kills One GAZA ...      0\n",
       "3  EU set to launch 'transit camps' EU ministers ...      0\n",
       "4  Earthquakes Shake Central Japan; Bullet Train ...      0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NEWS_TEST.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "e5IzjEK0Ixxl",
    "tags": []
   },
   "source": [
    "Train/Test Sets using GloVe embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7600, 2)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_data_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEWS_TRAIN = pd.DataFrame(news_data_train)[:10000]\n",
    "NEWS_TEST = pd.DataFrame(news_data_test)[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad dataset to a maximum review length in words\n",
    "MAX_LEN = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will embed the dataset into sequences of 100-dimension vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_glove_seq(review, max_len):\n",
    "  seq = np.zeros((max_len, 100))\n",
    "  for i, word in enumerate(review):\n",
    "    if i < max_len and word in glove_vectors:\n",
    "      seq[i] = glove_vectors[word]\n",
    "  return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize text reviews to numbers\n",
    "X_NEWS_TRAIN = np.zeros((len(NEWS_TRAIN), MAX_LEN, 100))\n",
    "X_NEWS_TEST = np.zeros((len(NEWS_TEST), MAX_LEN, 100))\n",
    "\n",
    "for i, row in NEWS_TRAIN.iterrows():\n",
    "  X_NEWS_TRAIN[i] = get_glove_seq(tokenize_and_clean(row['text'], stem_and_remove_stop_words=False), MAX_LEN)\n",
    "\n",
    "for i, row in NEWS_TEST.iterrows():\n",
    "  X_NEWS_TEST[i] = get_glove_seq(tokenize_and_clean(row['text'], stem_and_remove_stop_words=False), MAX_LEN)\n",
    "\n",
    "Y_NEWS_TRAIN = np.array(NEWS_TRAIN['label'])\n",
    "Y_NEWS_TEST = np.array(NEWS_TEST['label'])\n",
    "NUM_LABELS = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multinomial Regression - The model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "editable": true,
    "id": "i1-mK1-EPbQL",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-64776db849e2cffa",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# Defining neural network structure\n",
    "class MultinomialBoWClassifier(nn.Module):  # inheriting from nn.Module!\n",
    "  def __init__(self, max_word_len, embedding_dim, num_labels):\n",
    "    super(MultinomialBoWClassifier, self).__init__()\n",
    "    self.max_word_len = max_word_len\n",
    "    self.embedding_dim = embedding_dim\n",
    "    self.num_labels = num_labels\n",
    "\n",
    "    self.linear = nn.Linear(max_word_len * embedding_dim, num_labels)\n",
    "\n",
    "  def forward(self, x):\n",
    "    out = None\n",
    "    # flatten each individual sample while keeping the batch size dimension\n",
    "    x = x.view(x.size(0), -1)\n",
    "    out = self.linear(x)\n",
    "    out = nn.functional.softmax(out, dim=1)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "editable": false,
    "id": "_eVavHp7TOxv",
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "multibow_model = MultinomialBoWClassifier(max_word_len=MAX_LEN, embedding_dim=EMBEDDING_DIM, num_labels=NUM_LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "editable": false,
    "id": "wc0JD91dP9-a",
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "def train(model, x_train_data, y_train_data, epochs, batch_size, lr, weight_decay):\n",
    "  print('Training Started!')\n",
    "  optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  n_iter = len(x_train_data) // batch_size\n",
    "  print(n_iter, 'batches per epoch')\n",
    "\n",
    "  for epoch in range(epochs):\n",
    "    num_correct = 0\n",
    "    total_loss = 0.0\n",
    "    model.train()\n",
    "\n",
    "    for i in range(n_iter):\n",
    "      x, y = get_batch(i, batch_size, x_train_data, y_train_data)\n",
    "      x = x\n",
    "      y = y.long()\n",
    "\n",
    "      y_hat = model(x)\n",
    "      loss = criterion(y_hat, y)\n",
    "      total_loss += loss\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      if (epoch+1)%10 == 0 and (i+1) == n_iter:\n",
    "        print('epoch:', epoch+1,',loss =',loss.item(), ', training accuracy =',(y_hat.argmax(dim=1)==y).float().mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "editable": true,
    "id": "POVLJlC0_mEl",
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1000\n",
    "N_EPOCHS = 100\n",
    "LEARNING_RATE = 2e-3\n",
    "WEIGHT_DECAY = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "id": "uhVQB2RCbPj7",
    "scrolled": true,
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    train(multibow_model, X_NEWS_TRAIN, Y_NEWS_TRAIN, N_EPOCHS, BATCH_SIZE, lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "except:\n",
    "    print(\"Training failed. Please check your code.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_accuracy_mr(X_NEWS_TEST, Y_NEWS_TEST, multibow_model):\n",
    "    try:\n",
    "        multibow_model.eval()\n",
    "        with torch.no_grad():\n",
    "            text_vec = torch.tensor(X_NEWS_TEST, dtype=torch.float)\n",
    "            probs = multibow_model(text_vec)\n",
    "            pred = probs.argmax(dim=1)\n",
    "        targets = torch.tensor(Y_NEWS_TEST)\n",
    "        accuracy = (pred == targets).float().mean().item()\n",
    "        print(classification_report(targets, pred))\n",
    "        print('Accuracy: ', accuracy)\n",
    "    except Exception as e:\n",
    "        print('Error during execution of Test:')\n",
    "        # print traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.88      0.89      1900\n",
      "           1       0.93      0.97      0.95      1900\n",
      "           2       0.87      0.79      0.83      1900\n",
      "           3       0.83      0.88      0.85      1900\n",
      "\n",
      "    accuracy                           0.88      7600\n",
      "   macro avg       0.88      0.88      0.88      7600\n",
      "weighted avg       0.88      0.88      0.88      7600\n",
      "\n",
      "Accuracy:  0.8802631497383118\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "test_model_accuracy_mr(X_NEWS_TEST, Y_NEWS_TEST, multibow_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "bento_stylesheets": {
   "bento/extensions/flow/main.css": true,
   "bento/extensions/kernel_selector/main.css": true,
   "bento/extensions/kernel_ui/main.css": true,
   "bento/extensions/new_kernel/main.css": true,
   "bento/extensions/system_usage/main.css": true,
   "bento/extensions/theme/main.css": true
  },
  "captumWidgetMessage": {},
  "colab": {
   "gpuType": "T4",
   "private_outputs": true,
   "provenance": [
    {
     "file_id": "1TsKPn8cwghhFR-TwD0la25YRRzhw4XF9",
     "timestamp": 1685998969758
    }
   ]
  },
  "dataExplorerConfig": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "last_base_url": "https://11238.od.fbinfra.net/",
  "last_kernel_id": "8832c409-9272-44ac-a889-c233991c3bb0",
  "last_msg_id": "354bb3e2-f350a4bd64779ec7f6ce4382_4332",
  "last_server_session_id": "5625d4d2-18a1-473d-bf89-0f1a5ecc7c7b",
  "outputWidgetContext": {}
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
